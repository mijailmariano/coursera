{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43d2a8",
   "metadata": {},
   "source": [
    "### University of Michigan: Programming for Everyone\n",
    "    Module #3: Web Data\n",
    "    date: Saturday, June 25th 2022\n",
    "\n",
    "#### Web Data\n",
    "\n",
    "To represent the wide range of characters that computers must be able to handle  - we represent characters with more than \"one byte.\"\n",
    "\n",
    "- UTF-16: fixed length; two (2) bytes\n",
    "\n",
    "- UTF-32: fixed length; four (4) bytes\n",
    "- UTF-8: 1-4 bytes\n",
    "    - UTF-8 is recommended practice for encoding data to be exchanged between systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89a3a0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**When we read data from an external resource, we must decode it based on the caracter set so it is properly represented in Python 3 as a string:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec43ecf",
   "metadata": {},
   "source": [
    "![Python Strings to Bytes](images/web_data_01.jpg)\n",
    "\n",
    "- where \"data = mysock.recv(512)\" = bytes\n",
    "\n",
    "and \n",
    "\n",
    "- \"mystring = data.decode()\" = unicode\n",
    "\n",
    "**'decode()' method takes bytes and converts it to unicode (str)**\n",
    "<br>\n",
    "\n",
    "**'encode()' method takes strings (str) and converts it to bytes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef1bca",
   "metadata": {},
   "source": [
    "----\n",
    "\"import socket\" \n",
    "\n",
    "![HTTP Requests in Python](images/web_data_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bdc16",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\"D.R.Y\" = \"dont repeat yourself\" :)\n",
    "\n",
    "#### Using \"URLlib\" in Python\n",
    "\n",
    "Given that HTTP is so common - there is a library that can manage all the \"socket\" functions and can make web pages look like a file.\n",
    "\n",
    "- calling the module/library inside of python:\n",
    "\n",
    "    - import urllib.request, urlib.parse, urllib.error\n",
    "\n",
    "- example:\n",
    "  \n",
    "    - fhand = urlib.request.urlopen(\"<http://....>\") [where \"fhand\" stands for \"first handle\"][\"this line is similar to an 'open file' function\"]\n",
    "\n",
    "<b> example:\n",
    "\n",
    "for line in fhand:\n",
    "\n",
    "        print(line.decode().strip())\n",
    "\n",
    "**note: this syntax when printed will remove web page headers, but they are not deleted and may be called if needed.**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e65b3",
   "metadata": {},
   "source": [
    "#### Reading Web Pages continued\n",
    "\n",
    "![Google web scrapper](images/wd04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd69b84",
   "metadata": {},
   "source": [
    "#### next - we'll read through ea. line and decode into unicode and append to a dictionary.\n",
    "\n",
    "![Treat like File](images/web_data_03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90151829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# practicing the \"urllib\" import and functionality of this module/library to read files \n",
    "# urllib also has embedded \"socket\" code/syntax that makes this process more efficient and easier for us\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "# example\n",
    "\n",
    "fhandle = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "\n",
    "\n",
    "counts = dict()\n",
    "for line in fhandle:\n",
    "    words = line.decode().split() # \"splits\" ea. line in the file\n",
    "    for word in words: # iterating over every word in ea. individual line within the file\n",
    "        # we are \"appending\" ea. word as a \"key\" in the \"counts\" dictionary\n",
    "        # additionally, we are looking at ea. word and adding by 1 to the word-key \"value\" every time it is found within the line\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "# finally - we are printing the results of ea. \"key and value\" count pair for all words in the file\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380c848",
   "metadata": {},
   "source": [
    "### Understanding Web Scraping\n",
    "    Network Programs (Part 5)\n",
    "    date: Sunday, June 26th 2022\n",
    "\n",
    "![Web Scraping](images/wd05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e56cff",
   "metadata": {},
   "source": [
    "##### Why Scrape?\n",
    "\n",
    "Reasons may include:\n",
    "\n",
    "    1. Pulling data from the internet - particularly social data (i.e., \"who links to who?\")\n",
    "    2. Getting you own data back out of some systems/platforms that do not have \"exporting\" capabilities\n",
    "    3. To monitor a site for new/updating information \n",
    "    4. \"Spidering\" as scraping is sometimes called...in order to make a database for a search engine\n",
    "\n",
    "\n",
    "**NOTE: You should be very careful when scraping/spidering web sites**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae5c1e",
   "metadata": {},
   "source": [
    "----\n",
    "### In Summary - \n",
    "\n",
    "![module summary](images/wd06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\n\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6906c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "# example\n",
    "\n",
    "fhandle = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhandle:\n",
    "    words = line.decode().split() # \"splits\" ea. line in the file\n",
    "    for word in words: # iterating over every word in ea. individual line within the file\n",
    "        # we are \"appending\" ea. word as a \"key\" in the \"counts\" dictionary\n",
    "        # additionally, we are looking at ea. word and adding by 1 to the word-key \"value\" every time it is found within the line\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "# finally - we are printing the results of ea. \"key and value\" count pair for all words in the file\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = \"http://py4e-data.dr-chuck.net/comments_42.html\"\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "num_of_comments = 0\n",
    "counter = 0\n",
    "# spans = [int(tag.contents[0]) for tag in tags] / boolean masking to get all numbers in a list\n",
    "# sum_of_spans = sum(spans) / adding all the numbers in the boolean mask\n",
    "# print(sum_of_spans) / printing the total sum\n",
    "\n",
    "for tag in tags:\n",
    "    num_of_comments += 1\n",
    "    tag = int(tag.contents[0])\n",
    "    counter += tag\n",
    "\n",
    "print(num_of_comments)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cea1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://py4e-data.dr-chuck.net/comments_1495387.html\"\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "num_of_comments = 0\n",
    "counter = 0\n",
    "\n",
    "for tag in tags:\n",
    "    num_of_comments += 1\n",
    "    tag = int(tag.contents[0])\n",
    "    counter += tag\n",
    "\n",
    "print(num_of_comments)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474d981",
   "metadata": {},
   "source": [
    "----\n",
    "### Following Links in Python\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "\n",
    "Sequence of names: Fikret Montgomery Mhairade Butchi Anayah\n",
    "Last name in sequence: Anayah\n",
    "\n",
    "Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Unaiza.html\n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "\n",
    "Hint: The first character of the name of the last page that you will load is: R\n",
    "Strategy\n",
    "\n",
    "The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa5e57",
   "metadata": {},
   "source": [
    "----\n",
    "1. write a Python program that expands on http://www.py4e.com/code3/urllinks.py\n",
    "\n",
    "2. The program will use urllib to read the HTML from the data files below\n",
    "   \n",
    "3. extract the href = values from the anchor tags\n",
    "   \n",
    "4. scan for a tag that is in a particular position relative to the first name in the list \n",
    "   \n",
    "5. follow that link and repeat the process a number of times and...\n",
    "   \n",
    "6. report the ~~last~~ **name** in the link that is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl \n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "for i in range(7):\n",
    "    url = str(\"http://py4e-data.dr-chuck.net/known_by_Unaiza.html\")\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tags = soup(\"a\")\n",
    "    count = 0\n",
    "    name_lst = list()\n",
    "    for tag in tags:\n",
    "        count += 1\n",
    "        name_lst.append(tag.contents[0])\n",
    "        if count > 18:\n",
    "            break\n",
    "        url = tag.get(\"href\", None)\n",
    "        name = tag.contents[0]\n",
    "\n",
    "print(name_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "# from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl \n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = str(input(\"Enter web url: \"))\n",
    "num_of_repititions = int(input(\"Enter number of repititions: \"))\n",
    "ele_position = int(input(\"Enter link position: \"))\n",
    "\n",
    "for i in range(num_of_repititions):\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tags = soup(\"a\")\n",
    "    count = 0\n",
    "\n",
    "    for tag in tags:\n",
    "        count += 1\n",
    "        if count > ele_position:\n",
    "            break\n",
    "        url = tag.get(\"href\", None)\n",
    "        name = tag.contents[0]\n",
    "\n",
    "print(f'{name} is the last person in loop.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216bdee",
   "metadata": {},
   "source": [
    "----\n",
    "![data_on_the_web](images/wd07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e15f31",
   "metadata": {},
   "source": [
    "#### sending data across the net\n",
    "\n",
    "is like \"wire protocol\": meaning, **what we send on the \"wire\"**\n",
    "\n",
    "\"eXtensible Markup Language\" or **XML** is a wire format: (\"more robost\")\n",
    "**JSON** is a wire format: (\"lighter weight\")\n",
    "\n",
    "<person>\n",
    "\n",
    "<name>\n",
    "\n",
    "</name>\n",
    "\n",
    "<phone>\n",
    "\n",
    "</phone>\n",
    "\n",
    "</person>\n",
    "\n",
    "----\n",
    "**Internal Structure: is serialized **\n",
    "\n",
    "    - Python Dictionary\n",
    "\n",
    "**De-Serialize:**\n",
    "\n",
    "    - Java Hashmap\n",
    "\n",
    "![XML](images/wd08.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec02cbd4",
   "metadata": {},
   "source": [
    "<u>**XMLs are comprised of **</u>\n",
    "\n",
    "* \"Simple Elements\" \n",
    "  \n",
    "* \"Complex Elements\"\n",
    "\n",
    "**key features include:**\n",
    "\n",
    "    - start tag\n",
    "    - end tag\n",
    "    - text content \n",
    "    - attribute\n",
    "    - opening tag of XML\n",
    "    - self-closing tag \" />\"\n",
    "\n",
    "<u>**where:**</u>\n",
    "\n",
    "**\"TAGS\":** indicate the beginning and ending tag of XML\n",
    "\n",
    "**\"ATTRIBUTES\":** represent keyword/value pairs on the opening tag of XML\n",
    "\n",
    "**\"SERIALIZE/De-SERIALIZE\":** convert data in one program into a common format that can be stored and/or transmitted between systems in a programming language-independent manner\n",
    "\n",
    "    - de-serialization refers to receiving across the network and translating it back into \"readable text\"\n",
    "\n",
    "-----\n",
    "![text and attributes](images/wd09.jpg)\n",
    "\n",
    "\n",
    "XML as paths:\n",
    "\n",
    "**example:**\n",
    "\n",
    "    - <a> (parent folder)\n",
    "\n",
    "    - <b> X </b> (child folder)\n",
    "----\n",
    "\n",
    "XML Schemas\n",
    "\n",
    "![schemas](images/wd10.jpg)\n",
    "\n",
    "\n",
    "<u>**\"Validation\"**</u>\n",
    "\n",
    "*refers to contracts between applications and how they are developed and will typically be comprised of:*\n",
    "\n",
    "        1. XML Document \n",
    "        2. XML Schema Contract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc2870",
   "metadata": {},
   "source": [
    "**XSD Data types**\n",
    "\n",
    "Basic functions: \n",
    "\n",
    "- minOccurs\n",
    "\n",
    "- maxOccurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a30f8",
   "metadata": {},
   "source": [
    "![xsd_data_types](images/wd11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da11dd",
   "metadata": {},
   "source": [
    "----\n",
    "### Parsing XML in Python - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75112103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "data = ''' \n",
    "<person>\n",
    "    <name>Miguel</name>\n",
    "    <phone type=\"intl\">\n",
    "        +1 555 690 1516\n",
    "        </phone>\n",
    "        <email hide = \"yes\"/>\n",
    "</person>\n",
    "'''\n",
    "\n",
    "tree = ET.fromstring(data)\n",
    "print(\"Name:\", tree.find('name').text)\n",
    "print(\"Phone number:\", tree.find(\"phone\").text) # retriving the phone number from the \"phone\" tag within the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b050a",
   "metadata": {},
   "source": [
    "-----\n",
    "### Extracting Data from XML\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geoxml.py. \n",
    "\n",
    "1. The program will prompt for a URL, \n",
    "\n",
    "2. read the XML data from that URL using urllib and \n",
    "\n",
    "3. then parse and extract the comment counts from the XML data\n",
    "\n",
    "4. compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553)\n",
    "\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1495389.xml (Sum ends with 83)\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "Data Format and Approach\n",
    "The data consists of a number of names and comment counts in XML as follows:\n",
    "\n",
    "<comment>\n",
    "\n",
    "    <name>Matthias</name>\n",
    "\n",
    "    <count>97</count>\n",
    "\n",
    "</comment>\n",
    "\n",
    "* You are to look through all the <comment> tags and \n",
    "  \n",
    "* find the <count> values sum the numbers.\n",
    "  \n",
    "* The closest sample code that shows how to parse XML is geoxml.py. \n",
    "\n",
    "But since the nesting of the elements in our data is different than the data we are parsing in that sample code you will have to make real changes to the code.\n",
    "\n",
    "To make the code a little simpler, you can use an XPath selector string to look through the entire tree of XML for any tag named 'count' with the following line of code:\n",
    "\n",
    "counts = tree.findall('.//count')\n",
    "\n",
    "\n",
    "Take a look at the Python ElementTree documentation and look for the supported XPath syntax for details. You could also work from the top of the XML down to the comments node and then loop through the child nodes of the comments node.\n",
    "\n",
    "Sample Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The program will prompt for a URL\n",
    "# 2. read the XML data from that URL using urllib and \n",
    "# 3. then parse and extract the comment counts from the XML data\n",
    "# 4. compute the sum of the numbers in the file.\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from urllib.request import urlopen\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "\n",
    "url_address = input(\"Enter address: \")\n",
    "\n",
    "while True:\n",
    "    if len(url_address) < 1:\n",
    "        url_address = \"http://py4e-data.dr-chuck.net/comments_1495389.xml\"\n",
    "    print(\"Retrieving: \" + url_address)\n",
    "\n",
    "    xml = urllib.request.urlopen(url_address).read()\n",
    "    print(\"Retrieved: \" + str(len(xml)), \"characters\")\n",
    "\n",
    "    tree = ET.fromstring(xml)\n",
    "\n",
    "    counts =  tree.findall('.//count')\n",
    "    print(\"Count: \" + str(len(counts)))\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for count in counts:\n",
    "        counter += int(count.text)\n",
    "\n",
    "    print(\"Sum: \" + str(counter))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed094d59",
   "metadata": {},
   "source": [
    "----\n",
    "### Web Servies Part V: JavaScript Object Notation \"JSON\"\n",
    "\n",
    "JSON represents data as nested lists and dictionaries\n",
    "\n",
    "``Service Oriented Approach``\n",
    "\n",
    "- Application Program Interfaces (APIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d6489",
   "metadata": {},
   "source": [
    "![json](/images/wd12.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example question\n",
    "\n",
    "x = {\n",
    "    \"users\": [\n",
    "        {\n",
    "            \"status\": {\n",
    "                \"text\": \"@jazzychad I just bought one .__.\",\n",
    "             },\n",
    "             \"location\": \"San Francisco, California\",\n",
    "             \"screen_name\": \"leahculver\",\n",
    "             \"name\": \"Leah Culver\",\n",
    "         }]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c44205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[\"users\"][0][\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939465a",
   "metadata": {},
   "source": [
    "**``Week 6 Quiz``**\n",
    "1. Who is credited with getting the JSON movement started? \\\n",
    "(answer) Douglas Crockford\n",
    "\n",
    "2. What Python library do you have to import to pase and handle JSON? \\\n",
    "(answer) import JSON \n",
    "\n",
    "3. Which of the following is a web services approach used by the Twitter API? \\\n",
    "(answer) REST\n",
    "\n",
    "4. What kind of variable will you get in Python when the following JSON is parsed: \\\n",
    "   \n",
    "   { \"id\" : \"001\",\n",
    "  \"x\" : \"2\",\n",
    "  \"name\" : \"Chuck\"\n",
    "} \\\n",
    "(answer) A dictionary with three key/value pairs\n",
    "\n",
    "5. Which of the following is not true about the service-oriented approach? \\\n",
    "(answer) An application runs together all in one place\n",
    "\n",
    "6. If the following JSON were parsed and put into the variable x, \\\n",
    "   \n",
    "   x = {\n",
    "    \"users\": [\n",
    "        {\n",
    "            \"status\": {\n",
    "                \"text\": \"@jazzychad I just bought one .__.\",\n",
    "             },\n",
    "             \"location\": \"San Francisco, California\",\n",
    "             \"screen_name\": \"leahculver\",\n",
    "             \"name\": \"Leah Culver\",\n",
    "         }]} \\\n",
    "\n",
    "what Python code would extract \"Leah Culver\" from the JSON? \\\n",
    "(answer) x[\"users\"][0][\"name\"]\n",
    "\n",
    "7. What library call do you make to append properly encoded parameters to the end of a URL like the following: \\\n",
    "   http://maps.googleapis.com/maps/api/geocode/json?sensor=false&address=Ann+Arbor%2C+MI \\\n",
    "(answer) urllib.parse.urlencode()\n",
    "\n",
    "8. What happens when you exceed the Google geocoding API rate limit? \\\n",
    "(answer) You cannot use the API for 24 hours\n",
    "\n",
    "9. What protocol does Twitter use to protect its API?\n",
    "(answer) OAuth\n",
    "\n",
    "10. What header does Twitter use to tell you how many more API requests you can make before you will be rate limited?\n",
    "(answer) \"x-rate-limit-remaining\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd326048",
   "metadata": {},
   "source": [
    "**``Extracting Data from JSON``**\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/json2.py. \n",
    "\n",
    "1. The program will prompt for a URL\n",
    "2. read the JSON data from that URL using urllib and then...\n",
    "3. parse and extract the comment counts from the JSON data\n",
    "4. compute the sum of the numbers in the file and enter the sum below:\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "**Sample data:** http://py4e-data.dr-chuck.net/comments_42.json (Sum=2553) \\\n",
    "**Actual data:** http://py4e-data.dr-chuck.net/comments_1495390.json (Sum ends with 52)\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "\n",
    "Data Format \\\n",
    "The data consists of a number of names and comment counts in JSON as follows:\n",
    "\n",
    "{\n",
    "  comments: [\n",
    "    {\n",
    "      name: \"Matthias\"\n",
    "      count: 97\n",
    "    },\n",
    "    {\n",
    "      name: \"Geomer\"\n",
    "      count: 97\n",
    "    }\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "The closest sample code that shows how to parse JSON and extract a list is json2.py. You might also want to look at geoxml.py to see how to prompt for a URL and retrieve data from a URL.\n",
    "\n",
    "----\n",
    "\n",
    "**Sample Execution:**\n",
    "\n",
    "$ python3 solution.py \\\n",
    "Enter location: http://py4e-data.dr-chuck.net/comments_42.json \\\n",
    "Retrieving http://py4e-data.dr-chuck.net/comments_42.json \\\n",
    "Retrieved 2733 characters \\\n",
    "Count: 50 \\\n",
    "Sum: 2...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "url_address = input(\"Enter URL: \")\n",
    "\n",
    "while True:\n",
    "    if len(url_address) < 1:\n",
    "        url_address = \"http://py4e-data.dr-chuck.net/comments_1495390.json\"\n",
    "    print(\"Retrieving:\", url_address)\n",
    "    \n",
    "    uh = urllib.request.urlopen(url_address)\n",
    "    data = uh.read().decode()\n",
    "    \n",
    "    print(\"Retrieved:\", len(data), \"characters\")\n",
    "    json_object = json.loads(data)\n",
    "\n",
    "    sum = 0\n",
    "    total_sum = 0\n",
    "    \n",
    "    for comment in json_object[\"comments\"]:\n",
    "        sum += int(comment[\"count\"])\n",
    "        total_sum += 1\n",
    "\n",
    "    print(\"Count:\", total_sum)\n",
    "    print(\"Sum:\", str(sum))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017badde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
