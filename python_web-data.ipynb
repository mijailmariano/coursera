{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43d2a8",
   "metadata": {},
   "source": [
    "### University of Michigan: Programming for Everyone\n",
    "    Module #3: Web Data\n",
    "    date: Saturday, June 25th 2022\n",
    "\n",
    "#### Web Data\n",
    "\n",
    "To represent the wide range of characters that computers must be able to handle  - we represent characters with more than \"one byte.\"\n",
    "\n",
    "- UTF-16: fixed length; two (2) bytes\n",
    "\n",
    "- UTF-32: fixed length; four (4) bytes\n",
    "- UTF-8: 1-4 bytes\n",
    "    - UTF-8 is recommended practice for encoding data to be exchanged between systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89a3a0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**When we read data from an external resource, we must decode it based on the caracter set so it is properly represented in Python 3 as a string:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec43ecf",
   "metadata": {},
   "source": [
    "![Python Strings to Bytes](images/web_data_01.jpg)\n",
    "\n",
    "- where \"data = mysock.recv(512)\" = bytes\n",
    "\n",
    "and \n",
    "\n",
    "- \"mystring = data.decode()\" = unicode\n",
    "\n",
    "**'decode()' method takes bytes and converts it to unicode (str)**\n",
    "<br>\n",
    "\n",
    "**'encode()' method takes strings (str) and converts it to bytes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef1bca",
   "metadata": {},
   "source": [
    "----\n",
    "\"import socket\" \n",
    "\n",
    "![HTTP Requests in Python](images/web_data_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bdc16",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\"D.R.Y\" = \"dont repeat yourself\" :)\n",
    "\n",
    "#### Using \"URLlib\" in Python\n",
    "\n",
    "Given that HTTP is so common - there is a library that can manage all the \"socket\" functions and can make web pages look like a file.\n",
    "\n",
    "- calling the module/library inside of python:\n",
    "\n",
    "    - import urllib.request, urlib.parse, urllib.error\n",
    "\n",
    "- example:\n",
    "  \n",
    "    - fhand = urlib.request.urlopen(\"<http://....>\") [where \"fhand\" stands for \"first handle\"][\"this line is similar to an 'open file' function\"]\n",
    "\n",
    "<b> example:\n",
    "\n",
    "for line in fhand:\n",
    "\n",
    "        print(line.decode().strip())\n",
    "\n",
    "**note: this syntax when printed will remove web page headers, but they are not deleted and may be called if needed.**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e65b3",
   "metadata": {},
   "source": [
    "#### Reading Web Pages continued\n",
    "\n",
    "![Google web scrapper](images/wd04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd69b84",
   "metadata": {},
   "source": [
    "#### next - we'll read through ea. line and decode into unicode and append to a dictionary.\n",
    "\n",
    "![Treat like File](images/web_data_03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90151829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<h1>The': 1, 'First': 1, 'Page</h1>': 1, '<p>': 1, 'If': 1, 'you': 2, 'like,': 1, 'can': 1, 'switch': 1, 'to': 1, 'the': 1, '<a': 1, 'href=\"http://www.dr-chuck.com/page2.htm\">': 1, 'Second': 1, 'Page</a>.': 1, '</p>': 1}\n"
     ]
    }
   ],
   "source": [
    "# practicing the \"urllib\" import and functionality of this module/library to read files \n",
    "# urllib also has embedded \"socket\" code/syntax that makes this process more efficient and easier for us\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "# example\n",
    "\n",
    "fhandle = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "\n",
    "\n",
    "counts = dict()\n",
    "for line in fhandle:\n",
    "    words = line.decode().split() # \"splits\" ea. line in the file\n",
    "    for word in words: # iterating over every word in ea. individual line within the file\n",
    "        # we are \"appending\" ea. word as a \"key\" in the \"counts\" dictionary\n",
    "        # additionally, we are looking at ea. word and adding by 1 to the word-key \"value\" every time it is found within the line\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "# finally - we are printing the results of ea. \"key and value\" count pair for all words in the file\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380c848",
   "metadata": {},
   "source": [
    "### Understanding Web Scraping\n",
    "    Network Programs (Part 5)\n",
    "    date: Sunday, June 26th 2022\n",
    "\n",
    "![Web Scraping](images/wd05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e56cff",
   "metadata": {},
   "source": [
    "##### Why Scrape?\n",
    "\n",
    "Reasons may include:\n",
    "\n",
    "    1. Pulling data from the internet - particularly social data (i.e., \"who links to who?\")\n",
    "    2. Getting you own data back out of some systems/platforms that do not have \"exporting\" capabilities\n",
    "    3. To monitor a site for new/updating information \n",
    "    4. \"Spidering\" as scraping is sometimes called...in order to make a database for a search engine\n",
    "\n",
    "\n",
    "**NOTE: You should be very careful when scraping/spidering web sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7d8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with \"BeautifulSoup\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = input(\"Enter - \")\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# retrieving all of the anchor tags \n",
    "\n",
    "tags = soup(\"a\") # list of all 'achor tags' in the document/file \n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae5c1e",
   "metadata": {},
   "source": [
    "----\n",
    "### In Summary - \n",
    "\n",
    "![module summary](images/wd06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cc386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 400 Bad Request\n",
      "Date: Mon, 04 Jul 2022 13:27:35 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Content-Length: 308\n",
      "Connection: close\n",
      "Content-Type: text/html; charset=iso-8859-1\n",
      "\n",
      "<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n",
      "<html><head>\n",
      "<title>400 Bad Request</title>\n",
      "</head><body>\n",
      "<h1>Bad Request</h1>\n",
      "<p>Your browser sent a request that this server could not understand.<br />\n",
      "</p>\n",
      "<hr>\n",
      "<address>Apache/2.4.18 (Ubuntu) Server at do1.dr-chuck.com Port 80</address>\n",
      "</body></html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\n\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6906c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<h1>The': 1, 'First': 1, 'Page</h1>': 1, '<p>': 1, 'If': 1, 'you': 2, 'like,': 1, 'can': 1, 'switch': 1, 'to': 1, 'the': 1, '<a': 1, 'href=\"http://www.dr-chuck.com/page2.htm\">': 1, 'Second': 1, 'Page</a>.': 1, '</p>': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "# example\n",
    "\n",
    "fhandle = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhandle:\n",
    "    words = line.decode().split() # \"splits\" ea. line in the file\n",
    "    for word in words: # iterating over every word in ea. individual line within the file\n",
    "        # we are \"appending\" ea. word as a \"key\" in the \"counts\" dictionary\n",
    "        # additionally, we are looking at ea. word and adding by 1 to the word-key \"value\" every time it is found within the line\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "# finally - we are printing the results of ea. \"key and value\" count pair for all words in the file\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e02a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "2553\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = \"http://py4e-data.dr-chuck.net/comments_42.html\"\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "num_of_comments = 0\n",
    "counter = 0\n",
    "# spans = [int(tag.contents[0]) for tag in tags] / boolean masking to get all numbers in a list\n",
    "# sum_of_spans = sum(spans) / adding all the numbers in the boolean mask\n",
    "# print(sum_of_spans) / printing the total sum\n",
    "\n",
    "for tag in tags:\n",
    "    num_of_comments += 1\n",
    "    tag = int(tag.contents[0])\n",
    "    counter += tag\n",
    "\n",
    "print(num_of_comments)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3cea1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "2148\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://py4e-data.dr-chuck.net/comments_1495387.html\"\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "num_of_comments = 0\n",
    "counter = 0\n",
    "\n",
    "for tag in tags:\n",
    "    num_of_comments += 1\n",
    "    tag = int(tag.contents[0])\n",
    "    counter += tag\n",
    "\n",
    "print(num_of_comments)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474d981",
   "metadata": {},
   "source": [
    "----\n",
    "### Following Links in Python\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "\n",
    "Sequence of names: Fikret Montgomery Mhairade Butchi Anayah\n",
    "Last name in sequence: Anayah\n",
    "\n",
    "Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Unaiza.html\n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "\n",
    "Hint: The first character of the name of the last page that you will load is: R\n",
    "Strategy\n",
    "\n",
    "The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa5e57",
   "metadata": {},
   "source": [
    "----\n",
    "1. write a Python program that expands on http://www.py4e.com/code3/urllinks.py\n",
    "\n",
    "2. The program will use urllib to read the HTML from the data files below\n",
    "   \n",
    "3. extract the href = values from the anchor tags\n",
    "   \n",
    "4. scan for a tag that is in a particular position relative to the first name in the list \n",
    "   \n",
    "5. follow that link and repeat the process a number of times and...\n",
    "   \n",
    "6. report the ~~last~~ **name** in the link that is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4b9eab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Torrin', 'Derryn', 'Jakub', 'Kelam', 'Francesco', 'Toni', 'Emanuel', 'Luc', 'Helena', 'Buddy', 'Keane', 'Jayme', 'Teejay', 'Kealan', 'Kelum', 'Ishwar', 'Mercedez', 'Rachael', 'Keeman']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl \n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "for i in range(7):\n",
    "    url = str(\"http://py4e-data.dr-chuck.net/known_by_Unaiza.html\")\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tags = soup(\"a\")\n",
    "    count = 0\n",
    "    name_lst = list()\n",
    "    for tag in tags:\n",
    "        count += 1\n",
    "        name_lst.append(tag.contents[0])\n",
    "        if count > 18:\n",
    "            break\n",
    "        url = tag.get(\"href\", None)\n",
    "        name = tag.contents[0]\n",
    "\n",
    "print(name_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "308d20fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rorie\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "# from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl \n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = str(input(\"Enter web url: \"))\n",
    "num_of_repititions = int(input(\"Enter number of repititions: \"))\n",
    "ele_position = int(input(\"Enter link position: \"))\n",
    "\n",
    "for i in range(num_of_repititions):\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tags = soup(\"a\")\n",
    "    count = 0\n",
    "\n",
    "    for tag in tags:\n",
    "        count += 1\n",
    "        if count > ele_position:\n",
    "            break\n",
    "        url = tag.get(\"href\", None)\n",
    "        name = tag.contents[0]\n",
    "\n",
    "print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
