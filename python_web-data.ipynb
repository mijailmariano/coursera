{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43d2a8",
   "metadata": {},
   "source": [
    "### University of Michigan: Programming for Everyone\n",
    "    Module #3: Web Data\n",
    "    date: Saturday, June 25th 2022\n",
    "\n",
    "#### Web Data\n",
    "\n",
    "To represent the wide range of characters that computers must be able to handle  - we represent characters with more than \"one byte.\"\n",
    "\n",
    "- UTF-16: fixed length; two (2) bytes\n",
    "\n",
    "- UTF-32: fixed length; four (4) bytes\n",
    "- UTF-8: 1-4 bytes\n",
    "    - UTF-8 is recommended practice for encoding data to be exchanged between systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89a3a0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**When we read data from an external resource, we must decode it based on the caracter set so it is properly represented in Python 3 as a string:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec43ecf",
   "metadata": {},
   "source": [
    "![Python Strings to Bytes](images/web_data_01.jpg)\n",
    "\n",
    "- where \"data = mysock.recv(512)\" = bytes\n",
    "\n",
    "and \n",
    "\n",
    "- \"mystring = data.decode()\" = unicode\n",
    "\n",
    "**'decode()' method takes bytes and converts it to unicode (str)**\n",
    "<br>\n",
    "\n",
    "**'encode()' method takes strings (str) and converts it to bytes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef1bca",
   "metadata": {},
   "source": [
    "----\n",
    "\"import socket\" \n",
    "\n",
    "![HTTP Requests in Python](images/web_data_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bdc16",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\"D.R.Y\" = \"dont repeat yourself\" :)\n",
    "\n",
    "#### Using \"URLlib\" in Python\n",
    "\n",
    "Given that HTTP is so common - there is a library that can manage all the \"socket\" functions and can make web pages look like a file.\n",
    "\n",
    "- calling the module/library inside of python:\n",
    "\n",
    "    - import urllib.request, urlib.parse, urllib.error\n",
    "\n",
    "- example:\n",
    "  \n",
    "    - fhand = urlib.request.urlopen(\"<http://....>\") [where \"fhand\" stands for \"first handle\"][\"this line is similar to an 'open file' function\"]\n",
    "\n",
    "<b> example:\n",
    "\n",
    "for line in fhand:\n",
    "\n",
    "        print(line.decode().strip())\n",
    "\n",
    "**note: this syntax when printed will remove web page headers, but they are not deleted and may be called if needed.**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e65b3",
   "metadata": {},
   "source": [
    "#### Reading Web Pages continued\n",
    "\n",
    "![Google web scrapper](images/wd04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd69b84",
   "metadata": {},
   "source": [
    "#### next - we'll read through ea. line and decode into unicode and append to a dictionary.\n",
    "\n",
    "![Treat like File](images/web_data_03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90151829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<h1>The': 1, 'First': 1, 'Page</h1>': 1, '<p>': 1, 'If': 1, 'you': 2, 'like,': 1, 'can': 1, 'switch': 1, 'to': 1, 'the': 1, '<a': 1, 'href=\"http://www.dr-chuck.com/page2.htm\">': 1, 'Second': 1, 'Page</a>.': 1, '</p>': 1}\n"
     ]
    }
   ],
   "source": [
    "# practicing the \"urllib\" import and functionality of this module/library to read files \n",
    "# urllib also has embedded \"socket\" code/syntax that makes this process more efficient and easier for us\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "# example\n",
    "\n",
    "fhandle = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "\n",
    "\n",
    "counts = dict()\n",
    "for line in fhandle:\n",
    "    words = line.decode().split() # \"splits\" ea. line in the file\n",
    "    for word in words: # iterating over every word in ea. individual line within the file\n",
    "        # we are \"appending\" ea. word as a \"key\" in the \"counts\" dictionary\n",
    "        # additionally, we are looking at ea. word and adding by 1 to the word-key \"value\" every time it is found within the line\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "# finally - we are printing the results of ea. \"key and value\" count pair for all words in the file\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380c848",
   "metadata": {},
   "source": [
    "### Understanding Web Scraping\n",
    "    Network Programs (Part 5)\n",
    "    date: Sunday, June 26th 2022\n",
    "\n",
    "![Web Scraping](images/wd05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e56cff",
   "metadata": {},
   "source": [
    "##### Why Scrape?\n",
    "\n",
    "Reasons may include:\n",
    "\n",
    "    1. Pulling data from the internet - particularly social data (i.e., \"who links to who?\")\n",
    "    2. Getting you own data back out of some systems/platforms that do not have \"exporting\" capabilities\n",
    "    3. To monitor a site for new/updating information \n",
    "    4. \"Spidering\" as scraping is sometimes called...in order to make a database for a search engine\n",
    "\n",
    "\n",
    "**NOTE: You should be very careful when scraping/spidering web sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7d8fe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown url type: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mijailmariano/codeup-data-science/coursera/python_web-data.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mijailmariano/codeup-data-science/coursera/python_web-data.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mijailmariano/codeup-data-science/coursera/python_web-data.ipynb#ch0000010?line=3'>4</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter - \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mijailmariano/codeup-data-science/coursera/python_web-data.ipynb#ch0000010?line=4'>5</a>\u001b[0m html \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlopen(url)\u001b[39m.\u001b[39mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mijailmariano/codeup-data-science/coursera/python_web-data.ipynb#ch0000010?line=5'>6</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/urllib/request.py:501\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m, fullurl, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, timeout\u001b[39m=\u001b[39msocket\u001b[39m.\u001b[39m_GLOBAL_DEFAULT_TIMEOUT):\n\u001b[1;32m    499\u001b[0m     \u001b[39m# accept a URL or a Request object\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fullurl, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 501\u001b[0m         req \u001b[39m=\u001b[39m Request(fullurl, data)\n\u001b[1;32m    502\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m         req \u001b[39m=\u001b[39m fullurl\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/urllib/request.py:320\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[0;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{},\n\u001b[1;32m    318\u001b[0m              origin_req_host\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, unverifiable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    319\u001b[0m              method\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_url \u001b[39m=\u001b[39m url\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders \u001b[39m=\u001b[39m {}\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munredirected_hdrs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/urllib/request.py:346\u001b[0m, in \u001b[0;36mRequest.full_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_url \u001b[39m=\u001b[39m unwrap(url)\n\u001b[1;32m    345\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_url, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfragment \u001b[39m=\u001b[39m _splittag(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_url)\n\u001b[0;32m--> 346\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/urllib/request.py:375\u001b[0m, in \u001b[0;36mRequest._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype, rest \u001b[39m=\u001b[39m _splittype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_url)\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munknown url type: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_url)\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselector \u001b[39m=\u001b[39m _splithost(rest)\n\u001b[1;32m    377\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost:\n",
      "\u001b[0;31mValueError\u001b[0m: unknown url type: ''"
     ]
    }
   ],
   "source": [
    "# Working with \"BeautifulSoup\"\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input(\"Enter: \")\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# retrieving all of the anchor tags \n",
    "\n",
    "# tags = soup(\"a\") - list of all 'achor tags' in the document/file \n",
    "# for tag in tags:\n",
    "#     print(tag.get(\"href\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae5c1e",
   "metadata": {},
   "source": [
    "----\n",
    "### In Summary - \n",
    "\n",
    "![module summary](images/wd06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cc386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 400 Bad Request\n",
      "Date: Mon, 04 Jul 2022 13:27:35 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Content-Length: 308\n",
      "Connection: close\n",
      "Content-Type: text/html; charset=iso-8859-1\n",
      "\n",
      "<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n",
      "<html><head>\n",
      "<title>400 Bad Request</title>\n",
      "</head><body>\n",
      "<h1>Bad Request</h1>\n",
      "<p>Your browser sent a request that this server could not understand.<br />\n",
      "</p>\n",
      "<hr>\n",
      "<address>Apache/2.4.18 (Ubuntu) Server at do1.dr-chuck.com Port 80</address>\n",
      "</body></html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\n\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6906c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<h1>The': 1, 'First': 1, 'Page</h1>': 1, '<p>': 1, 'If': 1, 'you': 2, 'like,': 1, 'can': 1, 'switch': 1, 'to': 1, 'the': 1, '<a': 1, 'href=\"http://www.dr-chuck.com/page2.htm\">': 1, 'Second': 1, 'Page</a>.': 1, '</p>': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "# example\n",
    "\n",
    "fhandle = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhandle:\n",
    "    words = line.decode().split() # \"splits\" ea. line in the file\n",
    "    for word in words: # iterating over every word in ea. individual line within the file\n",
    "        # we are \"appending\" ea. word as a \"key\" in the \"counts\" dictionary\n",
    "        # additionally, we are looking at ea. word and adding by 1 to the word-key \"value\" every time it is found within the line\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "# finally - we are printing the results of ea. \"key and value\" count pair for all words in the file\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "2553\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = \"http://py4e-data.dr-chuck.net/comments_42.html\"\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "num_of_comments = 0\n",
    "counter = 0\n",
    "# spans = [int(tag.contents[0]) for tag in tags] / boolean masking to get all numbers in a list\n",
    "# sum_of_spans = sum(spans) / adding all the numbers in the boolean mask\n",
    "# print(sum_of_spans) / printing the total sum\n",
    "\n",
    "for tag in tags:\n",
    "    num_of_comments += 1\n",
    "    tag = int(tag.contents[0])\n",
    "    counter += tag\n",
    "\n",
    "print(num_of_comments)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cea1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "2148\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://py4e-data.dr-chuck.net/comments_1495387.html\"\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "num_of_comments = 0\n",
    "counter = 0\n",
    "\n",
    "for tag in tags:\n",
    "    num_of_comments += 1\n",
    "    tag = int(tag.contents[0])\n",
    "    counter += tag\n",
    "\n",
    "print(num_of_comments)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474d981",
   "metadata": {},
   "source": [
    "----\n",
    "### Following Links in Python\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "\n",
    "Sequence of names: Fikret Montgomery Mhairade Butchi Anayah\n",
    "Last name in sequence: Anayah\n",
    "\n",
    "Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Unaiza.html\n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "\n",
    "Hint: The first character of the name of the last page that you will load is: R\n",
    "Strategy\n",
    "\n",
    "The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa5e57",
   "metadata": {},
   "source": [
    "----\n",
    "1. write a Python program that expands on http://www.py4e.com/code3/urllinks.py\n",
    "\n",
    "2. The program will use urllib to read the HTML from the data files below\n",
    "   \n",
    "3. extract the href = values from the anchor tags\n",
    "   \n",
    "4. scan for a tag that is in a particular position relative to the first name in the list \n",
    "   \n",
    "5. follow that link and repeat the process a number of times and...\n",
    "   \n",
    "6. report the ~~last~~ **name** in the link that is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Torrin', 'Derryn', 'Jakub', 'Kelam', 'Francesco', 'Toni', 'Emanuel', 'Luc', 'Helena', 'Buddy', 'Keane', 'Jayme', 'Teejay', 'Kealan', 'Kelum', 'Ishwar', 'Mercedez', 'Rachael', 'Keeman']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl \n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "for i in range(7):\n",
    "    url = str(\"http://py4e-data.dr-chuck.net/known_by_Unaiza.html\")\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tags = soup(\"a\")\n",
    "    count = 0\n",
    "    name_lst = list()\n",
    "    for tag in tags:\n",
    "        count += 1\n",
    "        name_lst.append(tag.contents[0])\n",
    "        if count > 18:\n",
    "            break\n",
    "        url = tag.get(\"href\", None)\n",
    "        name = tag.contents[0]\n",
    "\n",
    "print(name_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d20fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rorie is the last person in loop.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "# from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl \n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = str(input(\"Enter web url: \"))\n",
    "num_of_repititions = int(input(\"Enter number of repititions: \"))\n",
    "ele_position = int(input(\"Enter link position: \"))\n",
    "\n",
    "for i in range(num_of_repititions):\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tags = soup(\"a\")\n",
    "    count = 0\n",
    "\n",
    "    for tag in tags:\n",
    "        count += 1\n",
    "        if count > ele_position:\n",
    "            break\n",
    "        url = tag.get(\"href\", None)\n",
    "        name = tag.contents[0]\n",
    "\n",
    "print(f'{name} is the last person in loop.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216bdee",
   "metadata": {},
   "source": [
    "----\n",
    "![data_on_the_web](images/wd07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e15f31",
   "metadata": {},
   "source": [
    "#### sending data across the net\n",
    "\n",
    "is like \"wire protocol\": meaning, **what we send on the \"wire\"**\n",
    "\n",
    "\"eXtensible Markup Language\" or **XML** is a wire format: (\"more robost\")\n",
    "**JSON** is a wire format: (\"lighter weight\")\n",
    "\n",
    "<person>\n",
    "\n",
    "<name>\n",
    "\n",
    "</name>\n",
    "\n",
    "<phone>\n",
    "\n",
    "</phone>\n",
    "\n",
    "</person>\n",
    "\n",
    "----\n",
    "**Internal Structure: is serialized **\n",
    "\n",
    "    - Python Dictionary\n",
    "\n",
    "**De-Serialize:**\n",
    "\n",
    "    - Java Hashmap\n",
    "\n",
    "![XML](images/wd08.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec02cbd4",
   "metadata": {},
   "source": [
    "<u>**XMLs are comprised of **</u>\n",
    "\n",
    "* \"Simple Elements\" \n",
    "  \n",
    "* \"Complex Elements\"\n",
    "\n",
    "**key features include:**\n",
    "\n",
    "    - start tag\n",
    "    - end tag\n",
    "    - text content \n",
    "    - attribute\n",
    "    - opening tag of XML\n",
    "    - self-closing tag \" />\"\n",
    "\n",
    "<u>**where:**</u>\n",
    "\n",
    "**\"TAGS\":** indicate the beginning and ending tag of XML\n",
    "\n",
    "**\"ATTRIBUTES\":** represent keyword/value pairs on the opening tag of XML\n",
    "\n",
    "**\"SERIALIZE/De-SERIALIZE\":** convert data in one program into a common format that can be stored and/or transmitted between systems in a programming language-independent manner\n",
    "\n",
    "    - de-serialization refers to receiving across the network and translating it back into \"readable text\"\n",
    "\n",
    "-----\n",
    "![text and attributes](images/wd09.jpg)\n",
    "\n",
    "\n",
    "XML as paths:\n",
    "\n",
    "**example:**\n",
    "\n",
    "    - <a> (parent folder)\n",
    "\n",
    "    - <b> X </b> (child folder)\n",
    "----\n",
    "\n",
    "XML Schemas\n",
    "\n",
    "![schemas](images/wd10.jpg)\n",
    "\n",
    "\n",
    "<u>**\"Validation\"**</u>\n",
    "\n",
    "*refers to contracts between applications and how they are developed and will typically be comprised of:*\n",
    "\n",
    "        1. XML Document \n",
    "        2. XML Schema Contract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc2870",
   "metadata": {},
   "source": [
    "**XSD Data types**\n",
    "\n",
    "Basic functions: \n",
    "\n",
    "- minOccurs\n",
    "\n",
    "- maxOccurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a30f8",
   "metadata": {},
   "source": [
    "![xsd_data_types](images/wd11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da11dd",
   "metadata": {},
   "source": [
    "----\n",
    "### Parsing XML in Python - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75112103",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
