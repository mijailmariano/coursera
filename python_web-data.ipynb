{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43d2a8",
   "metadata": {},
   "source": [
    "### University of Michigan: Programming for Everyone\n",
    "    Module #3: Web Data\n",
    "    date: Saturday, June 25th 2022\n",
    "\n",
    "#### Web Data\n",
    "\n",
    "To represent the wide range of characters that computers must be able to handle  - we represent characters with more than \"one byte.\"\n",
    "\n",
    "- UTF-16: fixed length; two (2) bytes\n",
    "\n",
    "- UTF-32: fixed length; four (4) bytes\n",
    "- UTF-8: 1-4 bytes\n",
    "    - UTF-8 is recommended practice for encoding data to be exchanged between systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89a3a0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**When we read data from an external resource, we must decode it based on the caracter set so it is properly represented in Python 3 as a string:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec43ecf",
   "metadata": {},
   "source": [
    "![Python Strings to Bytes](images/web_data_01.jpg)\n",
    "\n",
    "- where \"data = mysock.recv(512)\" = bytes\n",
    "\n",
    "and \n",
    "\n",
    "- \"mystring = data.decode()\" = unicode\n",
    "\n",
    "**'decode()' method takes bytes and converts it to unicode (str)**\n",
    "<br>\n",
    "\n",
    "**'encode()' method takes strings (str) and converts it to bytes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef1bca",
   "metadata": {},
   "source": [
    "----\n",
    "\"import socket\" \n",
    "\n",
    "![HTTP Requests in Python](images/web_data_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bdc16",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\"D.R.Y\" = \"dont repeat yourself\" :)\n",
    "\n",
    "#### Using \"URLlib\" in Python\n",
    "\n",
    "Given that HTTP is so common - there is a library that can manage all the \"socket\" functions and can make web pages look like a file.\n",
    "\n",
    "- calling the module/library inside of python:\n",
    "\n",
    "    - import urllib.request, urlib.parse, urllib.error\n",
    "\n",
    "- example:\n",
    "  \n",
    "    - fhand = urlib.request.urlopen(\"<http://....>\") [where \"fhand\" stands for \"first handle\"][\"this line is similar to an 'open file' function\"]\n",
    "\n",
    "<b> example:\n",
    "\n",
    "for line in fhand:\n",
    "\n",
    "        print(line.decode().strip())\n",
    "\n",
    "**note: this syntax when printed will remove web page headers, but they are not deleted and may be called if needed.**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd69b84",
   "metadata": {},
   "source": [
    "#### next - we'll read through ea. line and decode into unicode and append to a dictionary.\n",
    "\n",
    "![Treat like File](images/web_data_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e65b3",
   "metadata": {},
   "source": [
    "#### Reading Web Pages continued\n",
    "\n",
    "![Google web scrapper](images/wd04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90151829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<h1>The': 1, 'First': 1, 'Page</h1>': 1, '<p>': 1, 'If': 1, 'you': 2, 'like,': 1, 'can': 1, 'switch': 1, 'to': 1, 'the': 1, '<a': 1, 'href=\"http://www.dr-chuck.com/page2.htm\">': 1, 'Second': 1, 'Page</a>.': 1, '</p>': 1}\n"
     ]
    }
   ],
   "source": [
    "# practicing the \"urllib\" import and functionality of this module/library to read files \n",
    "# urllib also has embedded \"socket\" code/syntax that makes this process more efficient and easier for us\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "# example\n",
    "\n",
    "fhandle = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhandle:\n",
    "    words = line.decode().split() # \"splits\" ea. line in the file\n",
    "    for word in words: # iterating over every word in ea. individual line within the file\n",
    "        # we are \"appending\" ea. word as a \"key\" in the \"counts\" dictionary\n",
    "        # additionally, we are looking at ea. word and adding by 1 to the word-key \"value\" every time it is found within the line\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "# finally - we are printing the results of ea. \"key and value\" count pair for all words in the file\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380c848",
   "metadata": {},
   "source": [
    "### Understanding Web Scraping\n",
    "    Network Programs (Part 5)\n",
    "    date: Sunday, June 26th 2022\n",
    "\n",
    "![Web Scraping](images/wd05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e56cff",
   "metadata": {},
   "source": [
    "##### Why Scrape?\n",
    "\n",
    "Reasons may include:\n",
    "\n",
    "    1. Pulling data from the internet - particularly social data (i.e., \"who links to who?\")\n",
    "    2. Getting you own data back out of some systems/platforms that do not have \"exporting\" capabilities\n",
    "    3. To monitor a site for new/updating information \n",
    "    4. \"Spidering\" as scraping is sometimes called...in order to make a database for a search engine\n",
    "\n",
    "\n",
    "**NOTE: You should be very careful when scraping/spidering web sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae7d8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "# Working with \"BeautifulSoup\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = input(\"Enter - \")\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# retrieving all of the anchor tags \n",
    "\n",
    "tags = soup(\"a\") # list of all 'achor tags' in the document/file \n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae5c1e",
   "metadata": {},
   "source": [
    "----\n",
    "### In Summary - \n",
    "\n",
    "![module summary](images/wd06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616fe976",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
